{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x3DZNcl-HhnV"
   },
   "source": [
    "Playing Ms. Pac-Man with reinforcement learning\n",
    "> Josh Sowers\n",
    "\n",
    "> CSC 594 Section 801\n",
    "\n",
    "> Final Project\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X_g9tjsNHhc6"
   },
   "source": [
    "In the following notebook, I train an agent to play Ms. Pac-Man using OpenAI's gym and a convolution DDQN. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "quO3nHIkHhDR"
   },
   "source": [
    "# Classes and Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YV30dHZJxgKL"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "BckiXfzeVGBr",
    "outputId": "d38c50e6-7ce3-42e8-92b3-4139bf8b20d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "# First we are going to mount the drive to save the model quickly\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tSM3LYECWu7Z"
   },
   "outputs": [],
   "source": [
    "# Import os and change the directory to the correct folder\n",
    "import os\n",
    "os.chdir('/content/gdrive/My Drive/CSC594/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "id": "BT8a9AHU2Cqi",
    "outputId": "4b05a220-509e-4d97-fe13-693b1c8012e0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPool2D   # Note I do not use MaxPooling\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from google.colab import files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rbJa1noNHgsz"
   },
   "source": [
    "## Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mVh_wcqWx06O"
   },
   "source": [
    "### Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DMpGNtqY2KgW"
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=100000)  # This is currently using 100000 frames as a memory\n",
    "        \n",
    "        # Hyperparams\n",
    "        self.gamma = 0.95           # Discount rate, using slightly lower one to speed short term performance (wont have long term)\n",
    "        self.epsilon = 1.0          # Exploration rate\n",
    "        self.epsilon_min = 0.1      # Minimal exploration rate (this is for epsilon-greedy strategy\n",
    "        self.epsilon_decay = 0.995  # Decay rate for epsilon\n",
    "        self.update_rate = 10000    # Number of steps until updating the target network\n",
    "        \n",
    "        # Initialize double DQN and initializes both with same weights\n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        self.model.summary() \n",
    "\n",
    "    # Build the model\n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        \n",
    "        # Convolution layers first, don't want to lose any information by doing pooling \n",
    "        model.add(Conv2D(32, (6, 6), strides=4, padding='same', activation='relu', input_shape=self.state_size))      \n",
    "        model.add(Conv2D(64, (4, 4), strides=2, padding='same', activation='relu')) \n",
    "        model.add(Conv2D(64, (3, 3), strides=1, padding='same', activation='relu'))\n",
    "        model.add(Flatten())\n",
    "        # FC layers to learn\n",
    "        model.add(Dense(128, activation='relu'))\n",
    "        model.add(Dense(128, activation='relu'))\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        \n",
    "        model.compile(loss='mse', optimizer=Adam(lr=0.001))\n",
    "        return model\n",
    "\n",
    "    # Add to memory\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    # Do the action, epsilon-greedy\n",
    "    def act(self, state):\n",
    "        # Random\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        \n",
    "        # Policy\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])  \n",
    "\n",
    "    # Trains the model using randomly selected experiences in the replay memory\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        \n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            \n",
    "            if not done:\n",
    "                # Get the next state\n",
    "                max_action = np.argmax(self.model.predict(next_state)[0])\n",
    "                target = (reward + self.gamma * self.target_model.predict(next_state)[0][max_action])\n",
    "            else:\n",
    "                target = reward\n",
    "\n",
    "            # Predict Q-values\n",
    "            target_df = self.model.predict(state)\n",
    "            \n",
    "            # Update target with action using index\n",
    "            target_df[0][action] = target\n",
    "            \n",
    "            # Fit the model - MiniBatch Size will control training\n",
    "            self.model.fit(state, target_df, epochs=1, verbose=0)\n",
    "            \n",
    "        # Epsilon Decay    \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    #Updates target model\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "            \n",
    "    #Loads the model\n",
    "    def load_model(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    # Saves the model\n",
    "    def save_model(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bNSL626LHgB2"
   },
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "191Vqnv3HWZ4"
   },
   "outputs": [],
   "source": [
    "class Preprocessing:\n",
    "  # Please note that this preprocessing was adapted from github.com/ageron/tiny-dqn\n",
    "  # It was extremely helpful in getting the network to work\n",
    "\n",
    "    def process_frame(self, frame):\n",
    "        mspacman_color = np.array([210, 164, 74]).mean() # This gets Ms. Pac-Man's colour\n",
    "        img = frame[1:176:2, ::2]    # Crop and downsize - This gets rid of the bottom of the screen\n",
    "        img = img.mean(axis=2)       # Convert to greyscale\n",
    "        img[img==mspacman_color] = 0 # Improve contrast by making pacman white\n",
    "        img = (img - 128) / 128 - 1  # Normalize from -1 to 1.\n",
    "        \n",
    "        return np.expand_dims(img.reshape(88, 80, 1), axis=0)\n",
    "\n",
    "    def blend_images(self, images, blend):\n",
    "      # Create an empty image first\n",
    "      avg_image = np.expand_dims(np.zeros((88, 80, 1), np.float64), axis=0)\n",
    "\n",
    "      for image in images:\n",
    "          avg_image += image                # Add images\n",
    "          \n",
    "      if len(images) < blend:\n",
    "          return avg_image / len(images)    # Average of images if we don't have enough yet, so first N images where N < blend\n",
    "      else:\n",
    "          return avg_image / blend          # Otherwise blend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cAPkwmzgyEU5"
   },
   "source": [
    "# Envrionment and Playing the Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "id": "r7hK6EDuMaSR",
    "outputId": "8ecd80a7-9108-4042-857c-2787c4532c9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_7 (Conv2D)            (None, 22, 20, 32)        1184      \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 11, 10, 64)        32832     \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 11, 10, 64)        36928     \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 7040)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 128)               901248    \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 9)                 585       \n",
      "=================================================================\n",
      "Total params: 997,545\n",
      "Trainable params: 997,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Here we make the environment and initialize the agent and Preprocessing class\n",
    "env = gym.make('MsPacman-v0')\n",
    "state_size = (88, 80, 1)\n",
    "action_size = env.action_space.n\n",
    "agent = Agent(state_size, action_size)\n",
    "#agent.load_model('agent')\n",
    "\n",
    "preprocess = Preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "J57jjWvXDF56",
    "outputId": "f1579321-21c3-41d8-8cd9-881bc38a1475"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# This cell builds and saves the model architecture to a file\\nmodel = agent._build_model()\\nplot_model(model, to_file='model.png')\\nfiles.download('model.png')\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# This cell builds and saves the model architecture to a file\n",
    "model = agent._build_model()\n",
    "plot_model(model, to_file='model.png')\n",
    "files.download('model.png')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uEj-CZItCFu4"
   },
   "outputs": [],
   "source": [
    "# Global Variables\n",
    "episodes = 100  # Number of episodes\n",
    "batch_size = 32  # Batch Size for Network Updating\n",
    "skip_start = 90  # MsPacman-v0 waits for 90 actions before the episode begins. This is because the first 90 actions are the loading things for the game\n",
    "total_time = 0   # Counter for total number of steps taken\n",
    "avg_rewards = 0  # Used to compute avg reward over time\n",
    "blend = 4        # Number of images to blend\n",
    "done = False     # Initialize done to False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 647
    },
    "colab_type": "code",
    "id": "tSmzjh4bca_4",
    "outputId": "8e89238a-f1e0-485d-e831-224ea9f490b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1/100, game score: 200.0, reward: 200.0, avg reward: 200.0, time: 585, total time: 586\n",
      "episode: 2/100, game score: 510.0, reward: 510.0, avg reward: 355.0, time: 742, total time: 1329\n",
      "episode: 3/100, game score: 460.0, reward: 460.0, avg reward: 390.0, time: 728, total time: 2058\n",
      "episode: 4/100, game score: 240.0, reward: 240.0, avg reward: 352.5, time: 736, total time: 2795\n",
      "episode: 5/100, game score: 590.0, reward: 590.0, avg reward: 400.0, time: 783, total time: 3579\n",
      "episode: 6/100, game score: 110.0, reward: 110.0, avg reward: 351.6666666666667, time: 431, total time: 4011\n",
      "episode: 7/100, game score: 980.0, reward: 980.0, avg reward: 441.42857142857144, time: 907, total time: 4919\n",
      "episode: 8/100, game score: 640.0, reward: 640.0, avg reward: 466.25, time: 830, total time: 5750\n",
      "episode: 9/100, game score: 480.0, reward: 480.0, avg reward: 467.77777777777777, time: 642, total time: 6393\n",
      "episode: 10/100, game score: 440.0, reward: 440.0, avg reward: 465.0, time: 546, total time: 6940\n",
      "episode: 11/100, game score: 360.0, reward: 360.0, avg reward: 455.45454545454544, time: 807, total time: 7748\n",
      "episode: 12/100, game score: 530.0, reward: 530.0, avg reward: 461.6666666666667, time: 757, total time: 8506\n",
      "episode: 13/100, game score: 310.0, reward: 310.0, avg reward: 450.0, time: 632, total time: 9139\n",
      "episode: 14/100, game score: 640.0, reward: 640.0, avg reward: 463.57142857142856, time: 950, total time: 10090\n",
      "episode: 15/100, game score: 710.0, reward: 710.0, avg reward: 480.0, time: 1156, total time: 11247\n",
      "episode: 16/100, game score: 300.0, reward: 300.0, avg reward: 468.75, time: 623, total time: 11871\n",
      "episode: 17/100, game score: 450.0, reward: 450.0, avg reward: 467.6470588235294, time: 853, total time: 12725\n",
      "episode: 18/100, game score: 610.0, reward: 610.0, avg reward: 475.55555555555554, time: 860, total time: 13586\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-bc4e72febda1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-9ddf3c7bbcf1>\u001b[0m in \u001b[0;36mreplay\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     59\u001b[0m                 \u001b[0;31m# Get the next state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0mmax_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                 \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmax_action\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1399\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1400\u001b[0m                                             \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1401\u001b[0;31m                                             callbacks=callbacks)\n\u001b[0m\u001b[1;32m   1402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1403\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0mbatch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'batch'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'size'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'predict'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'begin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-1.15.0/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(episodes):\n",
    "\n",
    "    total_reward = 0\n",
    "    game_score = 0\n",
    "    state = preprocess.process_frame(env.reset())\n",
    "    images = deque(maxlen=blend)   # Get the images to blend\n",
    "    images.append(state)\n",
    "    \n",
    "    for skip in range(skip_start): # Skip the start of each game\n",
    "        env.step(0)\n",
    "    \n",
    "    for time in range(20000):      # Only letting it go 20,000 frames. The usual playthrough is 10 million from literature.\n",
    "        #env.render()              # Have to take out render for Colab\n",
    "        total_time += 1\n",
    "        \n",
    "        # Update target network every update_rate timesteps \n",
    "        if total_time % agent.update_rate == 0:\n",
    "            agent.update_target_model()\n",
    "        \n",
    "        # Return the avg of the last blend frames\n",
    "        state = preprocess.blend_images(images, blend)\n",
    "        \n",
    "        # Have the agent take a move\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        # Add to reward\n",
    "        game_score += reward\n",
    "        total_reward += reward\n",
    "        \n",
    "        # Return the average of the last 4 frames\n",
    "        next_state = preprocess.process_frame(next_state)\n",
    "        images.append(next_state)\n",
    "        next_state = preprocess.blend_images(images, blend)\n",
    "        \n",
    "        # Store in memory\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "        # Evaluate done state\n",
    "        if done:\n",
    "            avg_rewards += game_score\n",
    "            \n",
    "            print(\"episode: {}/{}, game score: {}, reward: {}, avg reward: {}, time: {}, total time: {}\"\n",
    "                  .format(i+1, episodes, game_score, total_reward, avg_rewards/(i+1), time, total_time))\n",
    "            if i % 10==0:\n",
    "                agent.save_model('agent')\n",
    "            break\n",
    "            \n",
    "        if len(agent.memory) > batch_size:\n",
    "            agent.replay(batch_size)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "594RLFP.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
